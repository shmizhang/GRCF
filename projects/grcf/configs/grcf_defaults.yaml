optimizer:
  params:
    eps: 1.0e-08
    lr: 2e-4
    weight_decay: 0
  type: Adam

dataset_config:
  cnmtdata:
    zoo_requirements:
    - cnmtdata.defaults
    features:
      train:
      - cnmt_data/open_images/detectron_fix_100/fc6/train,cnmt_data/m4c_textvqa_ocr_en_frcn_features/train_images
      val:
      - cnmt_data/open_images/detectron_fix_100/fc6/train,cnmt_data/m4c_textvqa_ocr_en_frcn_features/train_images
      test:
      - cnmt_data/open_images/detectron_fix_100/fc6/test,cnmt_data/m4c_textvqa_ocr_en_frcn_features/test_images
 
model_config:
  grcf:
    lr_scale_frcn: 0.1
    lr_scale_text_bert: 0.1
    lr_scale_mmt: 1.0  # no scaling
    obj:
      mmt_in_dim: 2048
      dropout_prob: 0.1
    ocr:
      mmt_in_dim: 3002  # 300 (FastText) + 604 (PHOC) + 2048 (Faster R-CNN) + 50 (all zeros; legacy)
      dropout_prob: 0.1
    mmt:
      hidden_size: 768
      num_hidden_layers: 4
    global_graph:
      hidden_size: 768
      num_hidden_layers: 2
      output_attentions: true
    classifier:
      type: linear
      ocr_max_num: 50
      ocr_ptr_net:
        hidden_size: 768
        query_key_size: 768
      params: {}
    model_data_dir: ${env.data_dir}
    metrics:
    #- type: anchor_acc
    #- type: graph_acc
    #- type: graph_precision
    #- type: graph_recall
    #- type: graph_f1
    - type: textcaps_bleu4
    #- type: first_pass
    losses:
    #- type: m4c_decoding_bce_with_mask
    - type: multi
      params:
      - type: m4c_decoding_bce_with_mask
        weight: 1
        params: {}
      - type: conf1_loss
        weight: 1
        params: {}
      - type: conf2_loss
        weight: 1
        params: {}
      - type:  gcn_ocr_cross_entropy
        weight: 1
        params: {}
      - type:  gcn_obj_cross_entropy
        weight: 1
        params: {}

evaluation:
  metrics:
  - textcaps_bleu4
  
training:
    clip_norm_mode: all
    clip_gradients: true
    max_grad_l2_norm: 0.25
    lr_scheduler: true
    #lr_steps:
    #- 6000
    #- 10000
    #lr_ratio: 0.1
    use_warmup: true
    warmup_factor: 0.2
    warmup_iterations: 1000
    #warmup_iterations: 1000
    max_iterations: 12000
    batch_size: 128
    num_workers: 1
    task_size_proportional_sampling: true
    early_stop:
      criteria: cnmtdata/textcaps_bleu4
      minimize: false
    seed: 36113173
